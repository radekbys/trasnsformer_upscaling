{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "98cda0e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import PIL\n",
    "import os\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision.transforms import v2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "887839d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, path, smallResolution, largeResolution):\n",
    "        super().__init__()\n",
    "        self.image_filenames = sorted(os.listdir(path))\n",
    "\n",
    "        self.data = []\n",
    "        for filename in self.image_filenames:\n",
    "            imagePath = os.path.join(path, filename)\n",
    "\n",
    "            transform = v2.Compose(\n",
    "                [v2.PILToTensor(), v2.ToDtype(torch.float32, scale=True)]\n",
    "            )\n",
    "\n",
    "            im = PIL.Image.open(imagePath)\n",
    "            X = im.resize(smallResolution)\n",
    "            X = transform(X)\n",
    "            y = im.resize(largeResolution)\n",
    "            y = transform(y)\n",
    "\n",
    "            self.data.append((X, y))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.data[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d1b4430",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainDataset = CustomDataset(\n",
    "    \"/home/radekbys/Code/trasnsformer_upscaling/dataset/DIV2K_train_HR/DIV2K_train_HR\",\n",
    "    (640, 360),\n",
    "    (1280, 720),\n",
    ")\n",
    "\n",
    "testDataset = CustomDataset(\n",
    "    \"/home/radekbys/Code/trasnsformer_upscaling/dataset/DIV2K_valid_HR/DIV2K_valid_HR\",\n",
    "    (640, 360),\n",
    "    (1280, 720),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "318b622c",
   "metadata": {},
   "source": [
    "# Building a vision transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "5bf902ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PatchEmbedding(nn.Module):\n",
    "    def __init__(self, img_size, patch_size, in_channels, embed_dim):\n",
    "        super().__init__()\n",
    "        self.patch_size = patch_size\n",
    "        self.img_size = img_size\n",
    "        self.grid_size = (img_size[0] // patch_size, img_size[1] // patch_size)\n",
    "        num_patches = self.grid_size[0] * self.grid_size[1]\n",
    "        \n",
    "        self.proj = nn.Conv2d(\n",
    "            in_channels=in_channels,\n",
    "            out_channels=embed_dim,\n",
    "            kernel_size=patch_size,\n",
    "            stride=patch_size,\n",
    "        )\n",
    "        self.pos_embed = nn.Parameter(torch.randn(1, num_patches, embed_dim))\n",
    "        \n",
    "    \n",
    "    def forward(self, x:torch.Tensor):\n",
    "        B=x.size(0)\n",
    "        x= self.proj(x) # (B, embed_dim, H//P, W//P)\n",
    "        x=x.flatten(2).transpose(1,2) # (B, N, embed_dim)\n",
    "        x = x + self.pos_embed\n",
    "        return x, self.grid_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "6cf38ee2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    def __init__(self, in_features, hidden_features, drop_rate):\n",
    "        super().__init__()\n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Linear(in_features=in_features, out_features=hidden_features),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(drop_rate),\n",
    "            nn.Linear(in_features=hidden_features, out_features=in_features),\n",
    "            nn.Dropout(drop_rate),\n",
    "        )\n",
    "\n",
    "    def forward(self, x: torch.Tensor):\n",
    "        return self.layers(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "399fcd7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerEncoderLayer(nn.Module):\n",
    "    def __init__(self, embed_dim, num_heads, mlp_dim, drop_rate):\n",
    "        super().__init__()\n",
    "        self.norm1 = nn.LayerNorm(embed_dim)\n",
    "        self.attention = nn.MultiheadAttention(embed_dim=embed_dim, num_heads=num_heads, dropout=drop_rate, batch_first=True)\n",
    "        self.norm2 = nn.LayerNorm(embed_dim)\n",
    "        self.mlp = MLP(in_features=embed_dim, hidden_features=mlp_dim, drop_rate=drop_rate)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = x+ self.attention(self.norm1(x), self.norm1(x), self.norm1(x))[0]\n",
    "        x = x + self.mlp(self.norm2(x))\n",
    "        return x\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "19a72fdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self, embed_dim, out_channels, upsample_factor):\n",
    "        super().__init__()\n",
    "        self.upsample = nn.Sequential(\n",
    "            nn.Conv2d(embed_dim, embed_dim, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Upsample(scale_factor=upsample_factor, mode='bilinear', align_corners=False),\n",
    "            nn.Conv2d(embed_dim, out_channels, kernel_size=3, padding=1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x, grid_size):\n",
    "        # x: (B, N, E)\n",
    "        B, N, E = x.shape\n",
    "        H, W = grid_size\n",
    "        x = x.transpose(1, 2).reshape(B, E, H, W)  # (B, E, H, W)\n",
    "        x = self.upsample(x)  # (B, 3, H*scale, W*scale)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "727cd37c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VisionTransformer(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        img_size,\n",
    "        patch_size,\n",
    "        embed_dim,\n",
    "        depth,\n",
    "        num_heads,\n",
    "        mlp_dim,\n",
    "        drop_rate=0.2,\n",
    "        in_channels=3,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.patchEmbed = PatchEmbedding(\n",
    "            img_size=img_size,\n",
    "            patch_size=patch_size,\n",
    "            in_channels=in_channels,\n",
    "            embed_dim=embed_dim,\n",
    "        )\n",
    "        self.encoder = nn.Sequential(\n",
    "            *[\n",
    "                TransformerEncoderLayer(\n",
    "                    embed_dim=embed_dim,\n",
    "                    num_heads=num_heads,\n",
    "                    mlp_dim=mlp_dim,\n",
    "                    drop_rate=drop_rate,\n",
    "                )\n",
    "                for _ in range(depth)\n",
    "            ]\n",
    "        )\n",
    "        self.norm = nn.LayerNorm(embed_dim)\n",
    "        self.decoder = Decoder(embed_dim=embed_dim, out_channels=3, upsample_factor=patch_size*2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x, grid_size = self.patchEmbed(x)  # (B, N, E)\n",
    "        x = self.encoder(x)  # (B, N, E)\n",
    "        x = self.norm(x)  # (B, N, E)\n",
    "        x = self.decoder(x, grid_size)  # (B, 3, H, W)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "0447d56c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 3, 720, 1280])\n"
     ]
    }
   ],
   "source": [
    "model = VisionTransformer(img_size=(368, 640), patch_size=10,\n",
    "                          embed_dim=512, depth=4, num_heads=8, mlp_dim=1024)\n",
    "\n",
    "image = torch.randn(1, 3, 368, 640)\n",
    "output = model(image)\n",
    "print(output.shape)  # Expected: (1, 3, 74, 1280)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
